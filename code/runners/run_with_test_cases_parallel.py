from concurrent.futures import ThreadPoolExecutor, as_completed
import torch
import pickle
import os
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
from dyna_gym.pipelines import uct_for_hf_transformer_pipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
import pandas as pd
import torch
from databench_eval import Runner, Evaluator, utils
from datasets import load_dataset
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
from databench_eval import Runner, Evaluator, utils
import os
import numpy as np
import json
import logging
import argparse
import ast
import traceback
 
logging.basicConfig(level=logging.INFO)
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

DEFAULT_OUTPUTDIR=f"/content/drive/MyDrive/TUE-WINTER-2024/CHALLENGES-CL/"
DEFAULT_TESTROOT=f"{DEFAULT_OUTPUTDIR}/test_cases"

DEFAULT_NUM_THREADS=2
START_IDX =None 
END_IDX = None
# maximum number of steps / tokens to generate in each episode
DEFAULT_HORIZON = 64  
DEFAULT_ROLLOUTS = 100

def parse_arguments():
    parser = argparse.ArgumentParser(description="Run QA pipeline in parallel")
    parser.add_argument('--output-dir', type=str, default=DEFAULT_OUTPUTDIR, help='Output directory')
    parser.add_argument('--test-root', type=str, default=DEFAULT_TESTROOT, help='Root directory for test cases')
    parser.add_argument('--horizon', type=int, default=DEFAULT_HORIZON, help='Horizon')
    parser.add_argument('--num_threads', type=int, default=DEFAULT_NUM_THREADS, help='Number of parallel threads')
    parser.add_argument('--start-idx', type=int, default=START_IDX, help='Start Index')
    parser.add_argument('--end-idx', type=int, default=END_IDX, help='End Index')
    parser.add_argument('--rollouts', type=int, default=DEFAULT_ROLLOUTS, help='Number of rollouts')
    return parser.parse_args()

def call_model(prompts, max_new_tokens=2500):
    """
    Tokenize prompt, model generate;
    prompts: str
    """
    inputs = tokenizer(prompts, return_tensors="pt").to(model.device)
    tokens = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.2,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    result = tokenizer.decode(tokens[0], skip_special_tokens=True)
    print(result)
    return result

def generate_dataframe_schma_json(df):
  schema = {
       "columns": [
           {"name": col, "type": str(df[col].dtype)}
           for col in df.columns
       ]
   }
  json_schema = json.dumps(schema, indent=4)
  return json_schema

def generate_dataframe_description_json(df):
  description = df.describe().to_json(orient='index', indent=4)
  return description

def generate_random_sample_of_n_rows_json(df, n=10):
    return df.sample(n=n).to_json(orient='records', indent=4)

def get_dataframe_by_id(df_id):
    parquet_file = f"hf://datasets/cardiffnlp/databench/data/{df_id}/all.parquet"
    print(f"Loading {parquet_file}")
    df = pd.read_parquet(parquet_file)
    return df

def prompt_generator(row, df):
    question = row['question']
    df_random_sample = '{}'
    if not row['dataset'] == "029_NYTimes":
       df_random_sample = generate_dataframe_description_json(df) 
    print(f"Generating:{question}, dataset:{row['dataset']}")
    prompt = f"""
# Instructions: Generate ONLY python code. Do not include explanations.  
# you can use pandas and numpy. Use the meta data information from df_schema, df_descprtion.
import pandas as pd
import numpy as np


# Description of dataframe schema.
df_schema = {generate_dataframe_schma_json(df)}

# Description of dataframe columns.
df_descrption = {generate_dataframe_description_json(df)}

# Randome sample of 10 rows from the dataframe.
df_random_sample = {df_random_sample}

# TODO: complete the following function in one line, by completing the return statement. It should give the answer to: How many rows are there in this dataframe?
def example(df: pd.DataFrame):
    df.columns=["A"]
    return df.shape[0]

# TODO: complete the following function in one line, by completing the return statement. It should give the answer to: {question}
def answer(df: pd.DataFrame):
    df.columns = {list(df.columns)}
    return"""
    return prompt


def post_process(response: str, dataset, dummy_df=None, use_dummy=False):
    """
    Process and execute the code generated by the model
    """
    try:
        df = dataset if not use_dummy else dummy_df
        lead = """
def answer(df):
    return """
        exec(
            "global ans\n"
            + lead
            + response.split("return")[2].split("\n")[0].strip().replace("[end of text]", "")
            + f"\nans = answer(df)"
        )
        return ans.split("\n")[0] if "\n" in str(ans) else ans
    except Exception as e:
        traceback.print_exc()
        return f"__CODE_ERROR__: {e}"
    


def compare(value, truth):
    """
    Compares the model output to the ground truth answer
    """
    return str(value).strip() == str(truth).strip()

def extract_functions_and_imports(code):
    """
    Extract all import statements and function definitions from the code.
    Returns a tuple:
    - List of import statements as strings.
    - Dictionary of function names and their evaluable strings.
    """
    # Parse the code into an AST
    parsed_code = ast.parse(code)

    # List to store import statements
    imports = []

    # Dictionary to store function names and their strings
    functions_map = {}

    for node in parsed_code.body:
        # Check for import statements
        if isinstance(node, ast.Import):
            imports.append(ast.unparse(node))
        elif isinstance(node, ast.ImportFrom):
            imports.append(ast.unparse(node))
        # Check for function definitions
        elif isinstance(node, ast.FunctionDef):
            function_name = node.name
            function_source = ast.unparse(node)
            functions_map[function_name] = function_source

    return imports, functions_map

def code_from_imports_function_map(imports, response_function_map, custom_answer=None):
  answer = response_function_map['answer'] if custom_answer is None else custom_answer
  preamble_template="\n".join(imports)
  code_to_run=preamble_template+"\n"+response_function_map['dummy_data']+"\n"+answer+"\n"+response_function_map['test_answer']+"\n"
  return code_to_run

def run_tests_for_answer(question_idx, sentence, prompt, model="Qwen/Qwen2.5-Coder-32B-Instruct", random_seed=42, test_root=DEFAULT_TESTROOT):
    """
    Runs a specific test case based on test_case files.
    """
    print(f"Running test for question {question_idx} with model {model} on sentence: {sentence}")
    completion = sentence[len(prompt):]
    return_statement = completion.split("\n")[0]
    print("Extracted return statement:", return_statement)
    method_template =f"""
def answer(df: pd.DataFrame):
    return {return_statement}
"""
    print("Method template:\n", method_template)
    imports, function_map = extract_functions_and_imports(method_template)

    test_file = f"{test_root}/dev/{model}/test_case_{question_idx}.py"
    if not os.path.exists(test_file):
        print(f"Not found test_case_{question_idx}.py for {model}")
        return False
    test_file_str = ""
    with open(test_file) as file:
        test_file_str = file.read()  # Read the content of the test file

    test_imports, test_function_map = extract_functions_and_imports(test_file_str)
    # Combine imports, dummy_data, answer, and test_answer functions
    code_to_run = code_from_imports_function_map(imports + test_imports, test_function_map, custom_answer=function_map["answer"])
    # Execute the code and check for errors
    try:
        exec(code_to_run, globals())  # Execute in the current scope
        test_answer(random_seed)  # Call the test_answer function
        return True
    except Exception as e:
        traceback.print_exc()
        return False

def error_detecting_reward_fn(question_idx, 
                              backing_df, 
                              prompt, 
                              test_root=DEFAULT_TESTROOT):

    def error_check(sentence):
        """
        Assign a reward based on the correctness of generated code.
        """
        pass_count = 0
        for model in ["Qwen/Qwen2.5-Coder-32B-Instruct", "nvidia/Llama-3.1-Nemotron-70B-Instruct", "meta-llama/Meta-Llama-3.1-70B-Instruct"]:
            max_tries = 3
            while max_tries > 0:
                max_tries -= 1
                try:
                    seed = np.random.randint(10000)
                    dummy_test_result = run_tests_for_answer(
                        question_idx, 
                        sentence,
                        prompt,
                        model=model, 
                        random_seed=seed,
                        test_root=test_root
                    )
                    if dummy_test_result:
                        pass_count += 1
                except Exception as e:
                    traceback.print_exc()
                    print(f"Runtime error: {e}")

        result = post_process(sentence, backing_df)
        if "CODE_ERROR" in str(result):
            print("CODE ERROR DETECTED")
            return -1
        elif pass_count > 0:
            print(f"PASSED A TEST! ({pass_count} times)")
            return 0.1 * pass_count
        else:
            return 0.1
    return error_check


# Hugging Face Model Generation Arguments
model_generation_args = dict(top_k=3,
    top_p=0.9,
    do_sample=True,
    temperature=0.2)

def fetch_all_dataframes(dataset):
  dataset_ids  = set(map(lambda qa: qa['dataset'],  dataset))
  retval = { ds_id: get_dataframe_by_id(ds_id) for ds_id in dataset_ids }
  return retval

def run_pipeline_on_qa_parallel(qa, dataset_map, 
                                                test_root=DEFAULT_TESTROOT, 
                                                output_dir=None,
                                                horizon=DEFAULT_HORIZON, 
                                                rollouts=100,
                                                num_threads=2, 
                                                start_idx=None, 
                                                end_idx=None):
    start_idx = start_idx if start_idx is not None else 0
    end_idx = end_idx if end_idx is not None else len(qa)

    if (start_idx < 0 or start_idx >= len(qa)) or (end_idx< 0 or end_idx >= len(qa)):
        print(f"Invalid start_idx or end_idx: {start_idx}, {end_idx}")
        return
    output_list = {}
    process_indices = range(start_idx, end_idx)  
    with ThreadPoolExecutor(max_workers=num_threads) as executor:  # Adjust max_workers based on your CPU capacity
        futures = {
            executor.submit(run_pipeline_on_qa_single, idx, qa[idx], dataset_map, 
                            test_root=test_root, output_dir=output_dir, horizon=horizon, rollouts=rollouts): idx
            for idx in process_indices 
        }

        for future in as_completed(futures):
            idx = futures[future]
            try:
                result = future.result()
                output_list[idx] = result
            except Exception as e:
                traceback.print_exc()
                print(f"Error processing QA {idx}: {e}")
    return output_list

def run_pipeline_on_qa_single(idx, qa_item, dataset_map,
                              test_root=DEFAULT_TESTROOT, 
                              output_dir=DEFAULT_OUTPUTDIR, 
                              horizon=DEFAULT_HORIZON
                              ,rollouts=100):
    if os.path.exists(f'{output_dir}/parallel-output_list-{idx}-06-01-2025.pkl'):
        print('SKIPPING')
        return None

    try:
        dataset_id = qa_item['dataset']
        backing_df = dataset_map[dataset_id]
        prompt = prompt_generator(qa_item, backing_df)
        pipeline = uct_for_hf_transformer_pipeline(
            model=model,
            tokenizer=tokenizer,
            horizon=horizon,
            reward_func=error_detecting_reward_fn(idx, backing_df, prompt, test_root=test_root),
            uct_args=dict(rollouts=rollouts, gamma=1, width=5, alg='p_uct'),
            model_generation_args=model_generation_args,
            should_plot_tree=False,
        )
        outputs = pipeline(input_str=prompt)
        result = {'texts': outputs['texts'], 'rewards': outputs['rewards'] }

        with open(f'{output_dir}/parallel-output_list-{idx}-06-01-2025.pkl', 'wb') as f:
            pickle.dump(result, f)
        return result
    except Exception as e:
        with open(f'{output_dir}/err-parallel-output_list-{idx}-06-01-2025.log', 'w') as f:
            f.write(f"Error: {e}")
        traceback.print_exc()
        print(e)
        return None

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
def model_and_tokenzier():
    if torch.cuda.is_available():
        from transformers import  BitsAndBytesConfig

        # Configure 8-bit quantization
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,  
            device_map="auto",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
        )
        logging.info("Quantization config: %s", quantization_config)
        print("Quantization Config: %s", quantization_config)

        # Load the tokenizer and model with quantization
        tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-Python-hf")
        model = AutoModelForCausalLM.from_pretrained(
            "codellama/CodeLlama-7b-Python-hf",
            #quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16,
        )
    else:
        tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-Python-hf")
        model = AutoModelForCausalLM.from_pretrained("codellama/CodeLlama-7b-Python-hf")
    return model, tokenizer


if __name__ == "__main__":
    args = parse_arguments()
    semeval_dev_qa = load_dataset("cardiffnlp/databench", name="semeval", split="dev")
    dataset_map = fetch_all_dataframes(semeval_dev_qa)
    model, tokenizer = model_and_tokenzier()
    vocab_size = tokenizer.vocab_size

    logging.info("Torch version: %s", torch.__version__)
    logging.info("Transformers version: %s", transformers.__version__)


    # python run_with_test_cases_parallel.py --output-dir "../output" --test-root "../output/test_cases" --horizon 64 --num_threads 5 --start-idx 0 --end-idx 319--rollouts 20 
    run_pipeline_on_qa_parallel(semeval_dev_qa, dataset_map, 
                                                    test_root=args.test_root, 
                                                    output_dir=args.output_dir, 
                                                    num_threads=args.num_threads, 
                                                    horizon=args.horizon,
                                                    rollouts=args.rollouts,
                                                    start_idx=args.start_idx, 
                                                    end_idx=args.end_idx)