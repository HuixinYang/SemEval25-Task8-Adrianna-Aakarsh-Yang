from concurrent.futures import ThreadPoolExecutor, as_completed
import torch
import pickle
import os
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
from dyna_gym.pipelines import uct_for_hf_transformer_pipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
import pandas as pd
import subprocess
import shlex
import zipfile
import torch
from databench_eval import Runner, Evaluator, utils
from datasets import load_dataset
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
from databench_eval import Runner, Evaluator, utils
import os
import numpy as np
import json
import logging

logging.basicConfig(level=logging.INFO)
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

DEFAULT_TESTROOT="/content/drive/MyDrive/TUE-WINTER-2024/CHALLENGES-CL/test_cases"

# Maximum number of steps / Tokens to generate in each episode
horizon = 512  


def call_model(prompts, max_new_tokens=2500):
    """
    Tokenize prompt, model generate;
    prompts: str
    """
    inputs = tokenizer(prompts, return_tensors="pt").to(model.device)
    tokens = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.2,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    result = tokenizer.decode(tokens[0], skip_special_tokens=True)
    print(result)
    return result

def generate_dataframe_schma_json(df):
  schema = {
       "columns": [
           {"name": col, "type": str(df[col].dtype)}
           for col in df.columns
       ]
   }
  json_schema = json.dumps(schema, indent=4)
  return json_schema

def generate_dataframe_description_json(df):
  description = df.describe().to_json(orient='index', indent=4)
  return description

def generate_dataframe_categorical_cols_json(df):
    categorical_data = {}
    for col_name in df.columns:
        if df[col_name].dtype == 'category' and len(df_all[col_name].unique()) < 300 and  len(df_all[col_name].unique()) > 0 :
            categorical_data[col_name] = df_all[col_name].unique().tolist()

    json_data = json.dumps(categorical_data, indent=4)
    return json_data

def generate_random_sample_of_n_rows_json(df, n=10):
    return df.sample(n=n).to_json(orient='records', indent=4)

def get_dataframe_by_id(df_id):
    parquet_file = f"hf://datasets/cardiffnlp/databench/data/{df_id}/all.parquet"
    print(f"Loading {parquet_file}")
    df = pd.read_parquet(parquet_file)
    return df

def prompt_generator(row, df):
    question = row['question']
    df_random_sample = '{}'
    if not row['dataset'] == "029_NYTimes":
       df_random_sample = generate_dataframe_description_json(df) 
    print(f"Generating:{question}, dataset:{row['dataset']}")
    prompt = f"""
# Instructions: Generate ONLY python code. Do not include explanations.  
# you can use pandas and numpy. Use the meta data information from df_schema, df_descprtion.
import pandas as pd
import numpy as np


# Description of dataframe schema.
df_schema = {generate_dataframe_schma_json(df)}

# Description of dataframe columns.
df_descrption = {generate_dataframe_description_json(df)}

# Randome sample of 10 rows from the dataframe.
df_random_sample = {df_random_sample}

# TODO: complete the following function in one line, by completing the return statement. It should give the answer to: How many rows are there in this dataframe?
def example(df: pd.DataFrame):
    df.columns=["A"]
    return df.shape[0]

# TODO: complete the following function in one line, by completing the return statement. It should give the answer to: {question}
def answer(df: pd.DataFrame):
    df.columns = {list(df.columns)}
    return"""
    return prompt


def post_process(response: str, dataset, dummy_df=None, use_dummy=False):
    """
    Process and execute the code generated by the model
    """
    try:
        df = dataset if not use_dummy else dummy_df
        lead = """
def answer(df):
    return """
        exec(
            "global ans\n"
            + lead
            + response.split("return")[2].split("\n")[0].strip().replace("[end of text]", "")
            + f"\nans = answer(df)"
        )
        return ans.split("\n")[0] if "\n" in str(ans) else ans
    except Exception as e:
        return f"__CODE_ERROR__: {e}"


def compare(value, truth):
    """
    Compares the model output to the ground truth answer
    """
    return str(value).strip() == str(truth).strip()


def run_tests_for_answer(question_idx, sentence, model="Qwen/Qwen2.5-Coder-32B-Instruct", random_seed=42, test_root=DEFAULT_TESTROOT):
    """
    Runs a specific test case based on test_case files.
    """
    test_file = f"{test_root}/dev/{model}/test_case_{question_idx}.py"
    if not os.path.exists(test_file):
        print(f"Test file not found: test_case_{question_idx}.py for {model}")
        return False

    with open(test_file) as file:
        test_file_str = file.read()

    try:
        exec(test_file_str, globals())  
        test_answer(random_seed)  
        return True
    except Exception as e:
        print(f"Error during test execution: {e}")
        return False


def error_detecting_reward_fn(question_idx, backing_df, test_root=DEFAULT_TESTROOT):
    def error_check(sentence):
        """
        Assign a reward based on the correctness of generated code.
        """
        pass_count = 0
        for model in ["Qwen/Qwen2.5-Coder-32B-Instruct", "nvidia/Llama-3.1-Nemotron-70B-Instruct", "meta-llama/Meta-Llama-3.1-70B-Instruct"]:
            max_tries = 3
            while max_tries > 0:
                max_tries -= 1
                try:
                    seed = np.random.randint(10000)
                    dummy_test_result = run_tests_for_answer(
                        question_idx, 
                        sentence, 
                        model=model, 
                        random_seed=seed,
                        test_root=test_root
                    )
                    if dummy_test_result:
                        pass_count += 1
                except Exception as e:
                    print(f"Runtime error: {e}")

        result = post_process(sentence, backing_df)
        if "CODE_ERROR" in str(result):
            print("CODE ERROR DETECTED")
            return -1
        elif pass_count > 0:
            print(f"PASSED A TEST! ({pass_count} times)")
            return 0.1 * pass_count
        else:
            return 0.1
    return error_check


# UCT Agent Arguments
uct_args = dict(
    rollouts=10,
    gamma=0.99,
    width=5,
    alg='p_uct',  
)

# Hugging Face Model Generation Arguments
model_generation_args = dict(
    top_k=3,
    top_p=0.9,
    do_sample=True,
    temperature=0.2,
)


# Example usage:
# run_pipeline_on_qa(qa)

def fetch_all_dataframes(dataset):
  dataset_ids  = set(map(lambda qa: qa['dataset'],  dataset))
  retval = { ds_id: get_dataframe_by_id(ds_id) for ds_id in dataset_ids }
  return retval


def run_pipeline_on_qa_parallel(qa, dataset_map, test_root=DEFAULT_TESTROOT):
    output_list = {}
    horizon = 512
    with ThreadPoolExecutor(max_workers=2) as executor:  # Adjust max_workers based on your CPU capacity
        futures = {
            executor.submit(run_pipeline_on_qa_single, idx, qa[idx], dataset_map, test_root=test_root): idx
            for idx in range(len(qa))
        }

        for future in as_completed(futures):
            idx = futures[future]
            try:
                result = future.result()
                output_list[idx] = result
            except Exception as e:
                print(f"Error processing QA {idx}: {e}")

    return output_list

def run_pipeline_on_qa_single(idx, qa_item, dataset_map,test_root=DEFAULT_TESTROOT):
    if os.path.exists(f'/content/drive/MyDrive/TUE-WINTER-2024/CHALLENGES-CL/parallel-output_list-{idx}-06-01-2025.pkl'):
        print('SKIPPING')
        return None

    try:
        dataset_id = qa_item['dataset']
        backing_df = dataset_map[dataset_id]
        prompt = prompt_generator(qa_item, backing_df)
        pipeline = uct_for_hf_transformer_pipeline(
            model=model,
            tokenizer=tokenizer,
            horizon=horizon,
            reward_func=error_detecting_reward_fn(idx, backing_df, test_root=test_root),
            uct_args=uct_args,
            model_generation_args=model_generation_args,
            should_plot_tree=False,
        )
        outputs = pipeline(input_str=prompt)
        result = {'texts': outputs['texts'], 'rewards': outputs['rewards'] }

        with open(f'/content/drive/MyDrive/TUE-WINTER-2024/CHALLENGES-CL/parallel-output_list-{idx}-06-01-2025.pkl', 'wb') as f:
            pickle.dump(result, f)

        return result
    except Exception as e:
        print(e)
        return None

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# check cuda avialble
print(torch.cuda.is_available())
def model_and_tokenzier():
    if torch.cuda.is_available():
        from transformers import  BitsAndBytesConfig

        # Configure 8-bit quantization
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,  
            device_map="auto",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
        )
        logging.info("Quantization config: %s", quantization_config)
        print("Quantization Config: %s", quantization_config)

        # Load the tokenizer and model with quantization
        tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-Python-hf")
        model = AutoModelForCausalLM.from_pretrained(
            "codellama/CodeLlama-7b-Python-hf",
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16,
        )
    else:
        tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-Python-hf")
        model = AutoModelForCausalLM.from_pretrained("codellama/CodeLlama-7b-Python-hf")
    return model, tokenizer

model, tokenizer = model_and_tokenzier()
vocab_size = tokenizer.vocab_size

logging.info("Torch version: %s", torch.__version__)
logging.info("Transformers version: %s", transformers.__version__)


# Example usage:
semeval_dev_qa = load_dataset("cardiffnlp/databench", name="semeval", split="dev")
dataset_map = fetch_all_dataframes(semeval_dev_qa)
# run_pipeline_on_qa(semeval_dev_qa, dataset_map)

run_pipeline_on_qa_parallel(semeval_dev_qa, dataset_map, test_root=DEFAULT_TESTROOT)

