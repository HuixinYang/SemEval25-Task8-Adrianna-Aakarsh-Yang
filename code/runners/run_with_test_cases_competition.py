from concurrent.futures import ThreadPoolExecutor, as_completed
import torch
import pickle
import os
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
from dyna_gym.pipelines import uct_for_hf_transformer_pipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
import pandas as pd
import torch
from databench_eval import Runner, Evaluator, utils
from datasets import load_dataset
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
from databench_eval import Runner, Evaluator, utils
import os
import numpy as np
import json
import logging
import argparse
import ast
import traceback

from prompting.test_case_runner import test_run_code
from prompting.test_case_load_dataset import load_phase_dataset
 
logging.basicConfig(level=logging.INFO)

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

DEFAULT_OUTPUTDIR=f"/content/drive/MyDrive/TUE-WINTER-2024/CHALLENGES-CL/"
DEFAULT_TESTROOT=f"{DEFAULT_OUTPUTDIR}/test_cases"

DEFAULT_NUM_THREADS=2
START_IDX =None 
END_IDX = None
# maximum number of steps / tokens to generate in each episode
DEFAULT_HORIZON = 64  
DEFAULT_ROLLOUTS = 100


def call_model(prompts, max_new_tokens=2500):
    """
    Tokenize prompt, model generate;
    prompts: str
    """
    inputs = tokenizer(prompts, return_tensors="pt").to(model.device)
    tokens = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.2,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    result = tokenizer.decode(tokens[0], skip_special_tokens=True)
    print(result)
    return result

def post_process(response: str, dataset, dummy_df=None, use_dummy=False):
    """
    Process and execute the code generated by the model
    """
    try:
        df = dataset if not use_dummy else dummy_df
        lead = """
def answer(df):
    return """
        exec(
            "global ans\n"
            + lead
            + response.split("return")[2].split("\n")[0].strip().replace("[end of text]", "")
            + f"\nans = answer(df)"
        )
        return ans.split("\n")[0] if "\n" in str(ans) else ans
    except Exception as e:
        traceback.print_exc()
        return f"__CODE_ERROR__: {e}"
    

def compare(value, truth):
    """
    Compares the model output to the ground truth answer
    """
    return str(value).strip() == str(truth).strip()

def extract_functions_and_imports(code):
    """
    Extract all import statements and function definitions from the code.
    Returns a tuple:
    - List of import statements as strings.
    - Dictionary of function names and their evaluable strings.
    """
    # Parse the code into an AST
    parsed_code = ast.parse(code)

    # List to store import statements
    imports = []

    # Dictionary to store function names and their strings
    functions_map = {}

    for node in parsed_code.body:
        # Check for import statements
        if isinstance(node, ast.Import):
            imports.append(ast.unparse(node))
        elif isinstance(node, ast.ImportFrom):
            imports.append(ast.unparse(node))
        # Check for function definitions
        elif isinstance(node, ast.FunctionDef):
            function_name = node.name
            function_source = ast.unparse(node)
            functions_map[function_name] = function_source

    return imports, functions_map

def code_from_imports_function_map(imports, response_function_map, custom_answer=None):
  answer = response_function_map['answer'] if custom_answer is None else custom_answer
  preamble_template="\n".join(imports)
  code_to_run=preamble_template+"\n"+response_function_map['dummy_data']+"\n"+answer+"\n"+response_function_map['test_answer']+"\n"
  return code_to_run

def run_tests_for_answer(question_idx, sentence, prompt, 
                         model="Qwen/Qwen2.5-Coder-32B-Instruct", 
                         random_seed=42, 
                         test=None,
                         test_root=DEFAULT_TESTROOT):
    """
    Runs a specific test case based on test_case files.
    """
    print(f"Running test for question {question_idx} with model {model} on sentence: {sentence}")
    completion = sentence[len(prompt):]
    return_statement = completion.split("\n")[0]
    print("Extracted return statement:", return_statement)
    method_template =f"""
def answer(df: pd.DataFrame):
    return {return_statement}
"""
    print("Method template:\n", method_template)
    imports, function_map = extract_functions_and_imports(method_template)
    # select
    test_file_str = test['content']
    test_imports, test_function_map = extract_functions_and_imports(test_file_str)
    # Combine imports, dummy_data, answer, and test_answer functions
    code_to_run = code_from_imports_function_map(imports + test_imports, test_function_map, custom_answer=function_map["answer"])
    # Execute the code and check for errors
    try:
        exec(code_to_run, globals())  # Execute in the current scope
        test_answer(random_seed)  # Call the test_answer function
        return True
    except Exception as e:
        traceback.print_exc()
        return False


def error_detecting_reward_fn(question_idx, backing_df, prompt, tests):

    def error_check(sentence):
        """
        Assign a reward based on the correctness of generated code.
        """
        pass_count = 0
        for test in tests: 
            max_tries = 3
            while max_tries > 0:
                max_tries -= 1
                try:
                    seed = np.random.randint(10000)
                    
                    # Parse the code into an AST
                    dummy_test_result = run_tests_for_answer(
                        question_idx, # Question index used to find the test.
                        sentence, # code generated by the model.
                        prompt,
                        model=model,  
                        random_seed=seed,
                        test=test # Where to find the test.
                    )

                    if dummy_test_result:
                        pass_count += 1
                except Exception as e:
                    traceback.print_exc()
                    print(f"Runtime error: {e}")
        result = post_process(sentence, backing_df)
        if "CODE_ERROR" in str(result):
            print("CODE ERROR DETECTED")
            return -1
        elif pass_count > 0:
            print(f"PASSED A TEST! ({pass_count} times)")
            return 0.1 * pass_count
        else:
            return 0.1
    return error_check


# Hugging Face Model Generation Arguments
model_generation_args = dict(top_k=3,
    top_p=0.9,
    do_sample=True,
    temperature=0.2)

def run_pipeline_on_qa_parallel(qa, dataset_map, prompt_dataset,test_dataset, 
                                    model, tokenizer,
                                    cache_dir=None,
                                    use_cache=True,
                                    regenerate=False,
                                    horizon=DEFAULT_HORIZON, 
                                    rollouts=100, 
                                    num_threads=2, 
                                    start_idx=None, 
                                    end_idx=None):

    start_idx = start_idx if start_idx is not None else 0
    end_idx = end_idx if end_idx is not None else (len(qa)-1)

    if (start_idx < 0 or start_idx >= len(qa)) or (end_idx< 0 or end_idx >= len(qa)):
        print(f"Invalid start_idx or end_idx: {start_idx}, {end_idx}")
        return
    output_list = {}
    process_indices = range(start_idx, end_idx)
    with ThreadPoolExecutor(max_workers=num_threads) as executor:  # Adjust max_workers based on your CPU capacity
        futures = {
            executor.submit(run_pipeline_on_qa_single, idx, qa[idx], dataset_map, 
                            test_dataset, prompt_dataset, 
                            model, tokenizer,
                            use_cache=use_cache,
                            regenerate=regenerate,
                            cache_dir=cache_dir,
                            horizon=horizon, 
                            rollouts=rollouts): idx
            for idx in process_indices 
        }

        for future in as_completed(futures):
            idx = futures[future]
            try:
                result = future.result()
                output_list[idx] = result
            except Exception as e:
                traceback.print_exc()
                print(f"Error processing QA {idx}: {e}")
    return output_list

def run_pipeline_on_qa_single(idx, qa_item, dataset_map, 
                              test_dataset, prompt_dataset, 
                              model, tokenizer,
                              use_cache=True,
                              regenerate=False,
                              cache_dir=DEFAULT_OUTPUTDIR, 
                              horizon=DEFAULT_HORIZON,
                              rollouts=100):

    output_dir = os.path.expanduser(f"{cache_dir}/pipeline-runs/{model}")

    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
    
    cache_file_path = f"{output_dir}/mct-result-semeval_id-{qa_item['semeval_id']}.parquet"
    if os.path.exists(f'{cache_file_path}') and regenerate is False:
        print('skipping')
        return None

    try:
        current_semeval_id = qa_item['semeval_id']
        dataset_id = qa_item['dataset']
        backing_df = dataset_map[dataset_id]

        # Find the prompt for this question
        prompt = prompt_dataset.filter(lambda x: int(x['semeval_id']) == int(current_semeval_id))[0]
        tests_for_questions = test_dataset.filter(lambda x: int(x['semeval_id']) == int(current_semeval_id))

        assert prompt['question'] == qa_item['question']
        prompt_content = prompt['content'] 

        logging.info(f"Question [idx:{idx}: semveal_id:{current_semeval_id}] {prompt['question']} Found {len(tests_for_questions)} tests")
        for test in tests_for_questions:
            logging.info(f"Test: {test['semeval_id']}")
            assert test['semeval_id'] == current_semeval_id
            assert test['question'] == qa_item['question']
        
        '''
        pipeline = uct_for_hf_transformer_pipeline(
            model=model,
            tokenizer=tokenizer,
            horizon=horizon,
            reward_func=error_detecting_reward_fn(idx, backing_df, prompt, tests=tests_for_questions),
            uct_args=dict(rollouts=rollouts, gamma=1, width=5, alg='p_uct'),
            model_generation_args=model_generation_args,
            should_plot_tree=False)
        outputs = pipeline(input_str=prompt)
        result = {'texts': outputs['texts'], 'rewards': outputs['rewards'] }
        '''
        
        result = None
        with open(f'{output_dir}/parallel-output_list-{idx}-06-01-2025.pkl', 'wb') as f:
            pickle.dump(result, f)
            
        return result
    except Exception as e:
        
        with open(f'{output_dir}/err-parallel-output_list-{idx}-06-01-2025.log', 'w') as f:
            f.write(f"Error: {e}")
            
        traceback.print_exc()
        print(e)
        
        return None

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
def model_and_tokenzier():
    if torch.cuda.is_available():
        from transformers import  BitsAndBytesConfig

        # Configure 8-bit quantization
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,  
            device_map="auto",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
        )
        logging.info("Quantization config: %s", quantization_config)
        print("Quantization Config: %s", quantization_config)

        # Load the tokenizer and model with quantization
        tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-Python-hf")
        model = AutoModelForCausalLM.from_pretrained(
            "codellama/CodeLlama-7b-Python-hf",
            #quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16,
        )
    else:
        tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-Python-hf")
        model = AutoModelForCausalLM.from_pretrained("codellama/CodeLlama-7b-Python-hf")
    return model, tokenizer

def parse_arguments():
    parser = argparse.ArgumentParser(description="Run QA pipeline in parallel")
    parser.add_argument('--output-dir', type=str, default=DEFAULT_OUTPUTDIR, help='Output directory')
    parser.add_argument('--test-root', type=str, default=DEFAULT_TESTROOT, help='Root directory for test cases')
    parser.add_argument('--horizon', type=int, default=DEFAULT_HORIZON, help='Horizon')
    parser.add_argument('--num_threads', type=int, default=DEFAULT_NUM_THREADS, help='Number of parallel threads')
    parser.add_argument('--start-idx', type=int, default=START_IDX, help='Start Index')
    parser.add_argument('--end-idx', type=int, default=END_IDX, help='End Index')
    parser.add_argument('--rollouts', type=int, default=DEFAULT_ROLLOUTS, help='Number of rollouts')
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_arguments()
    print(args)

    prompt_dataset = load_dataset('aakarsh-nair/semeval-2025-task-8-prompts-competition', split='dev')
    test_dataset = load_dataset('aakarsh-nair/semeval-2025-task-8-test-cases-competition', split='dev')

    logging.info(f"Loaded {len(prompt_dataset)} prompts and {len(test_dataset)} test cases") 
    #model, tokenizer = model_and_tokenzier() 
    model, tokenizer = (None, None)
    
    questions_dataset, dataset_map = load_phase_dataset(phase="competition", split="dev")
  
    run_pipeline_on_qa_parallel(questions_dataset, dataset_map, prompt_dataset, test_dataset, 
                                model, tokenizer, 
                                regenerate=True,
                                start_idx=args.start_idx, end_idx=args.end_idx)

    '''
    semeval_dev_qa = load_dataset("cardiffnlp/databench", name="semeval", split="dev")
    dataset_map = fetch_all_dataframes(semeval_dev_qa)
    model, tokenizer = model_and_tokenzier()
    vocab_size = tokenizer.vocab_size

    logging.info("Torch version: %s", torch.__version__)
    logging.info("Transformers version: %s", transformers.__version__)


    # python run_with_test_cases_parallel.py --output-dir "../output" --test-root "../output/test_cases" --horizon 64 --num_threads 5 --start-idx 0 --end-idx 319--rollouts 20 
    run_pipeline_on_qa_parallel(semeval_dev_qa, dataset_map, start_idx=args.start_idx, end_idx=args.end_idx)
    '''
    