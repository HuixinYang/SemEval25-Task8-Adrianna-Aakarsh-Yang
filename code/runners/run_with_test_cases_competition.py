from concurrent.futures import ThreadPoolExecutor, as_completed

from databench_eval import Runner, Evaluator, utils
from datasets import load_dataset
from dyna_gym.pipelines import uct_for_hf_transformer_pipeline
from prompting.test_case_runner import test_run_code
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer,AutoModel
import argparse
import ast
import json
import logging
import numpy as np
import os
import pandas as pd
import pickle
import torch
import traceback
import transformers
import pyarrow as pa
import pyarrow.parquet as pq
from cache_handler import cache_handler
 
from prompting.test_case_load_dataset import load_phase_dataset
 
logging.basicConfig(level=logging.INFO)

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

DEFAULT_OUTPUTDIR=f"/content/drive/MyDrive/TUE-WINTER-2024/CHALLENGES-CL/"
DEFAULT_TESTROOT=f"{DEFAULT_OUTPUTDIR}/test_cases"

DEFAULT_NUM_THREADS=2
START_IDX =None 
END_IDX = None
# maximum number of steps / tokens to generate in each episode
DEFAULT_HORIZON = 32  
DEFAULT_ROLLOUTS = 100


def call_model(prompts, max_new_tokens=2500):
    """
    Tokenize prompt, model generate;
    prompts: str
    """
    inputs = tokenizer(prompts, return_tensors="pt").to(model.device)
    tokens = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.2,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    result = tokenizer.decode(tokens[0], skip_special_tokens=True)
    print(result)
    return result

def post_process(response: str, dataset, dummy_df=None, use_dummy=False):
    """
    Process and execute the code generated by the model
    """
    try:
        df = dataset if not use_dummy else dummy_df
        lead = """
def answer(df):
    return """
        exec(
            "global ans\n"
            + lead
            + response.split("return")[2].split("\n")[0].strip().replace("[end of text]", "")
            + f"\nans = answer(df)"
        )
        return ans.split("\n")[0] if "\n" in str(ans) else ans
    except Exception as e:
        traceback.print_exc()
        return f"__CODE_ERROR__: {e}"
    

def compare(value, truth):
    """
    Compares the model output to the ground truth answer
    """
    return str(value).strip() == str(truth).strip()

def extract_functions_and_imports(code):
    """
    Extract all import statements and function definitions from the code.
    Returns a tuple:
    - List of import statements as strings.
    - Dictionary of function names and their evaluable strings.
    """
    # Parse the code into an AST
    parsed_code = ast.parse(code)

    # List to store import statements
    imports = []

    # Dictionary to store function names and their strings
    functions_map = {}

    for node in parsed_code.body:
        # Check for import statements
        if isinstance(node, ast.Import):
            imports.append(ast.unparse(node))
        elif isinstance(node, ast.ImportFrom):
            imports.append(ast.unparse(node))
        # Check for function definitions
        elif isinstance(node, ast.FunctionDef):
            function_name = node.name
            function_source = ast.unparse(node)
            functions_map[function_name] = function_source

    return imports, functions_map

def code_from_imports_function_map(imports, response_function_map, custom_answer=None):
  answer = response_function_map['answer'] if custom_answer is None else custom_answer
  preamble_template="\n".join(imports)
  code_to_run=preamble_template+"\n"+response_function_map['dummy_data']+"\n"+answer+"\n"+response_function_map['test_answer']+"\n"
  return code_to_run

def run_tests_for_answer(question_idx, sentence, prompt, 
                         model="Qwen/Qwen2.5-Coder-32B-Instruct", 
                         random_seed=42, 
                         test=None,
                         test_root=DEFAULT_TESTROOT):
    """
    Runs a specific test case based on test_case files.
    """
    print(f"Running test for question {question_idx} with model {model} ")
    completion = sentence[len(prompt):]
    return_statement = completion.split("\n")[0]
    print("Extracted return statement:>>>>>>>>>>", return_statement, "<<<<<<<<<<<<")
    method_template =f"""
def answer(df: pd.DataFrame):
    return {return_statement}
"""
    print("Method template:\n", method_template)
    imports, function_map = extract_functions_and_imports(method_template)
    # select
    test_file_str = test['content']
    test_imports, test_function_map = extract_functions_and_imports(test_file_str)
    # Combine imports, dummy_data, answer, and test_answer functions
    code_to_run = code_from_imports_function_map(imports + test_imports, test_function_map, custom_answer=function_map["answer"])
    # Execute the code and check for errors
    try:
        exec(code_to_run, globals())  # Execute in the current scope
        test_answer(random_seed)  # Call the test_answer function
        return True
    except Exception as e:
        traceback.print_exc()
        return False


def error_detecting_reward_fn(question_idx, backing_df, prompt, tests):

    def error_check(sentence):
        """
        Assign a reward based on the correctness of generated code.
        """
        pass_count = 0
        
        for test in tests: 
            max_tries = 3
            while max_tries > 0:
                max_tries -= 1
                try:
                    seed = np.random.randint(10000)
                    
                    print(f"Running test for question {question_idx} with model {test['model']} ")
                    
                    # Parse the code into an AST
                    dummy_test_result = run_tests_for_answer(
                        question_idx, # Question index used to find the test.
                        sentence, # code generated by the model.
                        prompt,
                        model=test["model"],  
                        random_seed=seed,
                        test=test # Where to find the test.
                    )

                    if dummy_test_result:
                        pass_count += 1
                except Exception as e:
                    traceback.print_exc()
                    print(f"Runtime error: {e}")
        result = post_process(sentence, backing_df)
        # TODO: ADD A PENALTY FOR EXCESS TOKENS AFTER NEWLINE
        if "CODE_ERROR" in str(result):
            print("CODE ERROR DETECTED")
            return -1
        elif pass_count > 0:
            print(f"PASSED A TEST! ({pass_count} times)")
            return 0.1 * pass_count
        else:
            return 0.1
        
    return error_check


# Hugging Face Model Generation Arguments
model_generation_args = dict(top_k=3,
    top_p=0.9,
    do_sample=True,
    temperature=0.2)

def run_pipeline_on_qa_parallel(qa, dataset_map, prompt_dataset,test_dataset, 
                                    model, tokenizer,
                                    cache_dir=None,
                                    use_cache=True,
                                    regenerate=False,
                                    horizon=32, 
                                    rollouts=100, 
                                    num_threads=2, 
                                    start_idx=None, 
                                    end_idx=None):

    start_idx = start_idx if start_idx is not None else 0
    end_idx = end_idx if end_idx is not None else (len(qa)-1)

    if (start_idx < 0 or start_idx >= len(qa)) or (end_idx< 0 or end_idx >= len(qa)):
        print(f"Invalid start_idx or end_idx: {start_idx}, {end_idx}")
        return
    output_list = {}
    process_indices = range(start_idx, end_idx)
    with ThreadPoolExecutor(max_workers=num_threads) as executor:  # Adjust max_workers based on your CPU capacity
        futures = {
            executor.submit(run_pipeline_on_qa_single, idx, qa[idx], dataset_map, 
                            test_dataset, prompt_dataset, 
                            model, tokenizer,
                            use_cache=use_cache,
                            regenerate=regenerate,
                            cache_dir=cache_dir,
                            horizon=horizon, 
                            rollouts=rollouts): idx
            for idx in process_indices 
        }

        for future in as_completed(futures):
            idx = futures[future]
            try:
                result = future.result()
                output_list[idx] = result
            except Exception as e:
                traceback.print_exc()
                print(f"Error processing QA {idx}: {e}")
    return output_list

def convert_pipeline_output_to_parquet(output):
    """
    Convert the pipeline output to a Parquet file format.
    This function takes the output of a pipeline, which is expected to be a dictionary
    containing 'texts' and 'rewards', and converts it into a PyArrow Table in Parquet format.
    Args:
        output (dict): A dictionary with two keys:
            - 'texts' (list of str): The list of text data.
            - 'rewards' (list of float): The list of reward values corresponding to the texts.
    Returns:
        pa.Table: A PyArrow Table containing the text and reward data.
    Example:
        output = {
            'texts': ["text1", "text2", "text3"],
            'rewards': [0.1, 0.2, 0.3]
        }
        table = convert_pipeline_output_to_parquet(output)
    """
    data = list(zip(output['texts'], output['rewards']))
    return pd.DataFrame(data, columns=['text', 'reward'])
    
    
@cache_handler(use_cache=True, 
               regenerate=True, 
               to_parquet_func=convert_pipeline_output_to_parquet)
def run_pipeline_on_qa_single(idx: int, 
                              qa_item: dict, 
                              dataset_map: dict, 
                              test_dataset , 
                              prompt_dataset, 
                              model: AutoModelForCausalLM, 
                              tokenizer: AutoTokenizer,
                              use_cache: bool=True,
                              regenerate: bool=False,
                              cache_dir: str ="~/.cache", 
                              horizon=32,
                              rollouts=100):
    try:
        # BEGIN: Extracting the prompt and test cases for the question
        current_semeval_id = qa_item['semeval_id']
        dataset_id = qa_item['dataset']
        backing_df = dataset_map[dataset_id]

        # Find the prompt for this question
        prompt = prompt_dataset.filter(lambda x: int(x['semeval_id']) == int(current_semeval_id))[0]
        tests_for_questions = test_dataset.filter(lambda x: int(x['semeval_id']) == int(current_semeval_id))

        assert prompt['question'] == qa_item['question']
        prompt_content = prompt['content'] 

        logging.info(f"Question [idx:{idx}: semveal_id:{current_semeval_id}] {prompt['question']} Found {len(tests_for_questions)} tests")

        for test in tests_for_questions:
            logging.info(f"Test: {test['semeval_id']}")
            assert test['semeval_id'] == current_semeval_id
            assert test['question'] == qa_item['question']
       # END: Extracting the prompt and test cases for the question 
      
        # BEGIN: Run Pipline function  
        pipeline = uct_for_hf_transformer_pipeline(
            model=model,
            tokenizer=tokenizer,
            horizon=horizon,
            reward_func=error_detecting_reward_fn(idx, backing_df, prompt_content, tests=tests_for_questions),
            uct_args=dict(rollouts=rollouts, gamma=1, width=5, alg='p_uct'),
            model_generation_args=model_generation_args,
            should_plot_tree=False)
        
        outputs = pipeline(input_str=prompt_content)
       
        return outputs
        # END: Save the results to a cache file
    except Exception as e:
        """
        with open(f'{output_dir}/err-parallel-output_list-{idx}-06-01-2025.log', 'w+') as f:
            f.write(f"Error: {e}")
        """             
        traceback.print_exc()
        print(e)
        return {"error": str(e), 
               'texts': [], 
               'rewards': []} 
 

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
def model_and_tokenzier():
    if torch.cuda.is_available():
        from transformers import  BitsAndBytesConfig

        # Configure 8-bit quantization
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,  
            device_map="auto",  
            trust_remote_code=True,  
            torch_dtype=torch.float16,  
        )
        logging.info("Quantization config: %s", quantization_config)
        print("Quantization Config: %s", quantization_config)

        # Load the tokenizer and model with quantization
        tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-Python-hf")
        model = AutoModelForCausalLM.from_pretrained(
            "codellama/CodeLlama-7b-Python-hf",
            #quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16,
        )
    else:
        tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-Python-hf")
        model = AutoModelForCausalLM.from_pretrained("codellama/CodeLlama-7b-Python-hf")
    return model, tokenizer

def parse_arguments():
    parser = argparse.ArgumentParser(description="Run QA pipeline in parallel")
    parser.add_argument('--output-dir', type=str, default=DEFAULT_OUTPUTDIR, help='Output directory')
    parser.add_argument('--test-root', type=str, default=DEFAULT_TESTROOT, help='Root directory for test cases')
    parser.add_argument('--horizon', type=int, default=DEFAULT_HORIZON, help='Horizon')
    parser.add_argument('--num_threads', type=int, default=DEFAULT_NUM_THREADS, help='Number of parallel threads')
    parser.add_argument('--start-idx', type=int, default=START_IDX, help='Start Index')
    parser.add_argument('--end-idx', type=int, default=END_IDX, help='End Index')
    parser.add_argument('--rollouts', type=int, default=DEFAULT_ROLLOUTS, help='Number of rollouts')
    return parser.parse_args()


def main(args):
    prompt_dataset = load_dataset('aakarsh-nair/semeval-2025-task-8-prompts-competition', 
                                  split='dev')
    test_dataset = load_dataset('aakarsh-nair/semeval-2025-task-8-test-cases-competition', 
                                  split='dev')
    logging.info(f"Loaded {len(prompt_dataset)} prompts and {len(test_dataset)} test cases") 
    model, tokenizer = model_and_tokenzier() 
    
    questions_dataset, dataset_map = load_phase_dataset(phase="competition", split="dev")
 
    run_pipeline_on_qa_single(0, questions_dataset[0], 
                              dataset_map, 
                              test_dataset, 
                              prompt_dataset, 
                              model, tokenizer, 
                              cache_dir=os.path.expanduser('~/.cache'),
                              regenerate=True)
    
if __name__ == "__main__":
    args = parse_arguments()
    print(args)
    main(args)
 
    '''
    run_pipeline_on_qa_parallel(questions_dataset, dataset_map, prompt_dataset, test_dataset, 
                                model, tokenizer, 
                                regenerate=True,
                                start_idx=args.start_idx, end_idx=args.end_idx)
    '''
    '''
    semeval_dev_qa = load_dataset("cardiffnlp/databench", name="semeval", split="dev")
    dataset_map = fetch_all_dataframes(semeval_dev_qa)
    model, tokenizer = model_and_tokenzier()
    vocab_size = tokenizer.vocab_size

    logging.info("Torch version: %s", torch.__version__)
    logging.info("Transformers version: %s", transformers.__version__)


    # python run_with_test_cases_parallel.py --output-dir "../output" --test-root "../output/test_cases" --horizon 64 --num_threads 5 --start-idx 0 --end-idx 319--rollouts 20 
    run_pipeline_on_qa_parallel(semeval_dev_qa, dataset_map, start_idx=args.start_idx, end_idx=args.end_idx)
    '''
    