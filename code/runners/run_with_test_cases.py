import torch
import pickle
import os
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# Maximum number of steps / Tokens to generate in each episode
horizon = 512  

# Configure 8-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,  
    device_map="auto",  
    trust_remote_code=True,  
    torch_dtype=torch.float16,  
)

# Load the tokenizer and model with quantization
tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-Python-hf")
model = AutoModelForCausalLM.from_pretrained(
    "codellama/CodeLlama-7b-Python-hf",
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.float16,
)

vocab_size = tokenizer.vocab_size


def call_model(prompts, max_new_tokens=64):
    """
    Tokenize prompt, model generate;
    prompts: str
    """
    inputs = tokenizer(prompts, return_tensors="pt").to(model.device)
    tokens = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.2,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    result = tokenizer.decode(tokens[0], skip_special_tokens=True)
    print(result)
    return result


def prompt_generator(row):
    """
    Generates a Python code completion prompt from a dataframe row.
    """
    question = row['question']
    df = row['df']
    prompt = f"""
# TODO: complete the following function in one line. It should give the answer to: How many rows are there in this dataframe?
def example(df: pd.DataFrame):
    df.columns=["A"]
    return df.shape[0]

# TODO: complete the following function in one line. It should give the answer to: {question}
def answer(df: pd.DataFrame):
    df.columns = {list(df.columns)}
    return"""
    return prompt


def post_process(response: str, dataset, dummy_df=None, use_dummy=False):
    """
    Process and execute the code generated by the model
    """
    try:
        df = dataset if not use_dummy else dummy_df
        lead = """
def answer(df):
    return """
        exec(
            "global ans\n"
            + lead
            + response.split("return")[2].split("\n")[0].strip().replace("[end of text]", "")
            + f"\nans = answer(df)"
        )
        return ans.split("\n")[0] if "\n" in str(ans) else ans
    except Exception as e:
        return f"__CODE_ERROR__: {e}"


def compare(value, truth):
    """
    Compares the model output to the ground truth answer
    """
    return str(value).strip() == str(truth).strip()


def run_tests_for_answer(question_idx, sentence, model="Qwen/Qwen2.5-Coder-32B-Instruct", random_seed=42):
    """
    Runs a specific test case based on test_case files.
    """
    test_file = f"/content/drive/MyDrive/TUE-WINTER-2024/CHALLENGES-CL/test_cases/{model}/test_case_{question_idx}.py"
    if not os.path.exists(test_file):
        print(f"Test file not found: test_case_{question_idx}.py for {model}")
        return False

    with open(test_file) as file:
        test_file_str = file.read()

    try:
        exec(test_file_str, globals())  
        test_answer(random_seed)  
        return True
    except Exception as e:
        print(f"Error during test execution: {e}")
        return False


def error_detecting_reward_fn(question_idx):
    def error_check(sentence):
        """
        Assign a reward based on the correctness of generated code.
        """
        pass_count = 0
        for model in ["Qwen/Qwen2.5-Coder-32B-Instruct"]:
            max_tries = 3
            while max_tries > 0:
                max_tries -= 1
                try:
                    seed = np.random.randint(10000)
                    dummy_test_result = run_tests_for_answer(
                        question_idx, sentence, model=model, random_seed=seed
                    )
                    if dummy_test_result:
                        pass_count += 1
                except Exception as e:
                    print(f"Runtime error: {e}")

        result = post_process(sentence, df_all)
        if "CODE_ERROR" in str(result):
            print("CODE ERROR DETECTED")
            return -1
        elif pass_count > 0:
            print(f"PASSED A TEST! ({pass_count} times)")
            return 0.1 * pass_count
        else:
            return 0.1
    return error_check


# UCT Agent Arguments
uct_args = dict(
    rollouts=25,
    gamma=0.99,
    width=5,
    alg='p_uct',  
)

# Hugging Face Model Generation Arguments
model_generation_args = dict(
    top_k=3,
    top_p=0.9,
    do_sample=True,
    temperature=0.2,
)


def run_pipeline_on_qa(qa):
    """
    Run the full pipeline on a set of QA pairs
    """
    output_list = []
    horizon = 256
    for idx in range(len(qa)):
        print("-" * 20, idx, "-" * 20)
        prompt = prompt_generator(qa[idx])
        pipeline = uct_for_hf_transformer_pipeline(
            model=model,
            tokenizer=tokenizer,
            horizon=horizon,
            reward_func=error_detecting_reward_fn(idx),
            uct_args=uct_args,
            model_generation_args=model_generation_args,
            should_plot_tree=False,
        )
        outputs = pipeline(input_str=prompt)
        output_list.append((outputs['texts'], outputs['rewards']))

        with open(f'/content/drive/MyDrive/TUE-WINTER-2024/CHALLENGES-CL/output_list-{idx}.pkl', 'wb') as f:
            pickle.dump(output_list, f)
    return output_list


# Example usage:
# qa = [{"question": "How many rows are in this dataframe?", "df": pd.DataFrame({"A": [1, 2, 3]})}]
# run_pipeline_on_qa(qa)
