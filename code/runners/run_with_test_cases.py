import torch
import pickle

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# Maximum number of steps / Tokens to generate in each episode
horizon = 512  

# Configure 8-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,  # Load in 8-bit precision
    device_map="auto",  # Automatically distribute the model across available devices
    trust_remote_code=True,  # Trust remote code for this model
    torch_dtype=torch.float16,  # Use float16 data type for computation
)

# Load the tokenizer and model with quantization
tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-Python-hf")
model = AutoModelForCausalLM.from_pretrained(
      "codellama/CodeLlama-7b-Python-hf",
      quantization_config=quantization_config,
      device_map="auto",
      trust_remote_code=True,
      torch_dtype=torch.float16,
  )
vocab_size = tokenizer.vocab_size




def call_model(prompts, max_new_tokens=64):
    """
    tokenize prompt, model generate;
    prompts: str
    """
    # print("*" * 50)
    inputs = tokenizer(prompts, return_tensors="pt").to(model.device)
    tokens = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.2,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id)
    result = tokenizer.decode(tokens[0], skip_special_tokens=True)
    print(result)
    return result


def prompt_generatoa((ow):
    question = row['question']
    df = df_all
    prompt = f"""
# TODO: complete the following function in one line. It should give the answer to: How many rows are there in this dataframe?
def example(df: pd.DataFrame):
    df.columns=["A"]
    return df.shape[0]

# TODO: complete the following function in one line. It should give the answer to: {question}
def answer(df: pd.DataFrame):
    df.columns = {list(df.columns)}
    return"""
    return prompt


def post_process(response: str, dataset, dummy_df=None, use_dummy=False):
    """
    process and execute the code generated by model
    response: the textual completion generated by the model
    dataset: the dataset we want to query, in this one data point case is "050_ING" which is df_all(I load it locally) above;
    """

    """
    Below is how this process actually look like
    df = pd.DataFrame({1:"aa",2:"bb"})
    response = "def print():\n return print(hello world)"
    global ans
    def answer(df):
        return response.split("return")[2].split("\n")[0].strip().replace("[end of text]", "")
    ans = answer(df)  #ans is the predicted textual answer of the question by model, which should match with the gold answer;
    """

    try:
        # df = loader(dataset)
        df = dataset if not use_dummy else dummy_df
        # TODO need to use the AST here.
        lead = """
def answer(df):
    return """
        exec(
            "global ans\n"
            + lead
            + response.split("return")[2]
            .split("\n")[0]
            .strip()
            .replace("[end of text]", "")
            + f"\nans = answer(df)"
        )
        # no true result is > 1 line atm, needs 1 line for txt format
        return ans.split("\n")[0] if "\n" in str(ans) else ans
    except Exception as e:
        return f"__CODE_ERROR__: {e}"


def compare(value, truth):
    return str(value).strip() == str(truth).strip()


def run_tests_for_answer(question_idx, sentence,
                         model="Qwen/Qwen2.5-Coder-32B-Instruct",
                         random_seed=42):
  imports, function_map = extract_functions_and_imports(sentence)
  test_file = f"/content/drive/MyDrive/TUE-WINTER-2024/CHALLENGES-CL/test_cases/{model}/test_case_{question_idx}.py"
  # return false if file does not exist.
  if not os.path.exists(test_file):
    print(f"Not found test_case_{question_idx}.py for {model}")
    return False
  test_file_str = ""
  with open(test_file) as file:
    test_file_str = file.read()  # Read the content of the test file

  test_imports, test_function_map = extract_functions_and_imports(test_file_str)
  # Combine imports, dummy_data, answer, and test_answer functions
  code_to_run = code_from_imports_function_map(imports + test_imports, test_function_map, custom_answer=function_map["answer"])
  # Execute the code and check for errors
  try:
    exec(code_to_run, globals())  # Execute in the current scope
    test_answer(random_seed)  # Call the test_answer function
    return True
  except Exception as e:
    return False


def error_detecting_reward_fn(question_idx):
    def error_check(sentence):
      """
      if (get correct NL answer) -> 1
      else -> 0
      """
      debug=True
      #print(f"SENTENCE:[{sentence}]")
      pass_count = 0
      for model in ["Qwen/Qwen2.5-Coder-32B-Instruct"]:
        max_tries = 3
        while max_tries > 0:
          max_tries -= 1
          try:
            seed = np.random.randint(10000)
            dummy_test_result = run_tests_for_answer(question_idx,
                                                     sentence,
                                                     model=model,
                                                     random_seed=seed)
            if dummy_test_result:
              pass_count+=1
          except Exception as e:
            print("runtime error",e)

      result = post_process(sentence, df_all)
      #print(f"Comparing [{result}] with [{qa[question_idx]['answer']}]")
      if str(result).find("CODE_ERROR") >0:
          if debug: print("CODE ERROR DETECTED")
          return -1
      elif pass_count > 0 :
          print(f"PASSED A TEST !!! {pass_count}")
          return .1 * pass_count
      else: # compiles but not runnable
          return 0.1
    return error_check


# arguments for the UCT agent
uct_args = dict(
    rollouts=25,
    gamma=0.99,
    width=5,
    alg='p_uct',  # or p_uct
)

# will be passed to huggingface model.generate()
model_generation_args = dict(
    top_k=3,
    top_p=0.9,
    do_sample=True,
    temperature=0.2,
)


def run_pipeline_on_qa(qa):
  output_list = []
  horizon=256
  for idx in [319]: #range(len(qa)):
    print("-"*20, idx, "-"*20)
    prompt = prompt_generator(qa[idx])
    #print("prompt", prompt)
    pipeline = uct_for_hf_transformer_pipeline(
        model=model,
        tokenizer=tokenizer,
        horizon=horizon,
        reward_func=error_detecting_reward_fn(idx),
        uct_args=uct_args,
        model_generation_args=model_generation_args,
        should_plot_tree=False,  # plot the tree after generation
    )
    outputs = pipeline(input_str=prompt)
    output_list.append((outputs['texts'], outputs['rewards']))
    # progressively save output_lists to pickle file.
    with open(f'/content/drive/MyDrive/TUE-WINTER-2024/CHALLENGES-CL/output_list-{idx}.pkl', 'wb') as f:
      pickle.dump(output_list, f)
  return output_list


run_pipeline_on_qa(qa)
