import ast
import numpy as np
import traceback
import logging

def generate_method_template(return_statement):
    """
    Generates a method template string for a function named 'answer' that takes a pandas DataFrame as input and returns a specified return statement.
    Args:
        return_statement (str): The return statement to be included in the generated method template.
    Returns:
        str: A string representing the method template with the specified return statement.
    """
    method_template = f"""
def answer(df: pd.DataFrame):
    return {return_statement}
"""
    return method_template

#TODO: Remove this hacky crap.
def post_process(response: str, dataset, dummy_df=None, use_dummy=False):
    """
    Process and execute the code generated by the model
    """
    logging.debug(f"Post processing response: {response}")
    try:
        df = dataset if not use_dummy else dummy_df
        lead = """
def answer(df):
    return """
        exec_code = "global ans\n" + lead \
            + response.split("return")[1].split("\n")[0].strip().replace("[end of text]", "") \
            + f"\nans = answer(df)"
        print(f"POST PROCESSING EXEC CODE: {exec_code}")
        exec(exec_code)
        return ans.split("\n")[0] if "\n" in str(ans) else ans
    except Exception as e:
        traceback.print_exc()
        return f"__CODE_ERROR__: {e} Code:\n{response}"


def extract_functions_and_imports(code):
    """
    Extract all import statements and function definitions from the code.
    Returns a tuple:
    - List of import statements as strings.
    - Dictionary of function names and their evaluable strings.
    """
    # Parse the code into an AST
    parsed_code = ast.parse(code)

    # List to store import statements
    imports = []

    # Dictionary to store function names and their strings
    functions_map = {}

    for node in parsed_code.body:
        # Check for import statements
        if isinstance(node, ast.Import):
            imports.append(ast.unparse(node))
        elif isinstance(node, ast.ImportFrom):
            imports.append(ast.unparse(node))
        # Check for function definitions
        elif isinstance(node, ast.FunctionDef):
            function_name = node.name
            function_source = ast.unparse(node)
            functions_map[function_name] = function_source

    return imports, functions_map

def code_from_imports_function_map(imports, response_function_map, custom_answer=None):
    """
    Generates a code string by combining provided imports, dummy data, and answer functions.

    Args:
        imports (list of str): A list of import statements to be included in the code.
        response_function_map (dict): A dictionary containing keys 'dummy_data', 'answer', and 'test_answer' 
                                    with corresponding code snippets as values.
        custom_answer (str, optional): A custom answer function to replace the default 'answer' from 
                                    response_function_map. Defaults to None.

    Returns:
        str: A string representing the complete code to be executed, including imports, dummy data, 
            the answer function, and the test answer function.
    """
    answer = response_function_map['answer'] if custom_answer is None else custom_answer
    preamble_template="\n".join(imports)
    code_to_run=preamble_template+"\n"+response_function_map['dummy_data']+"\n"+answer+"\n"+response_function_map['test_answer']+"\n"
    return code_to_run

def evaluate_return_statement(return_statement, df):
    """
    Evaluates a return statement using the provided pandas DataFrame.
    Args:
        return_statement (str): The return statement to be evaluated.
        df (pd.DataFrame): The pandas DataFrame to be used in the evaluation.
    Returns:
        Any: The result of evaluating the return statement.
    """
    try:
        answer_method = generate_method_template(return_statement) 
        answer = post_process(answer_method, df)
        return answer
    except Exception as e:
        traceback.print_exc()
        return f"__RUNTIME_ERROR__: {e}"
    
def extract_return_statement(sentence, prompt):
    """
    Extracts the return statement from a given sentence based on a provided prompt.
    Args:
        sentence (str): The complete sentence from which the return statement needs to be extracted.
        prompt (str): The initial part of the sentence that precedes the return statement.
    Returns:
        str: The extracted return statement from the sentence.
    """
    completion = sentence[len(prompt):]
    return_statement = completion.split("\n")[0]
    return return_statement

def run_tests_for_answer(question_idx, sentence, prompt, random_seed=42, test=None):
    """
    Runs a specific test case based on test_case files.
    """
    logging.debug(f"Running test for question {question_idx}")
    return_statement = extract_return_statement(sentence, prompt)
    logging.debug(f"Extracted return statement:>>>>>>>>>>{return_statement} <<<<<<<<<<<<")
    method_template = generate_method_template(return_statement)
    logging.debug(f"Method template:\n{ method_template}")
    imports, function_map = extract_functions_and_imports(method_template)
    # select
    test_file_str = test['content']
    test_imports, test_function_map = extract_functions_and_imports(test_file_str)
    # Combine imports, dummy_data, answer, and test_answer functions
    code_to_run = code_from_imports_function_map(imports + test_imports, test_function_map, custom_answer=function_map["answer"])
    # Execute the code and check for errors
    try:
        exec(code_to_run, globals())  # Execute in the current scope
        test_answer(random_seed)  # Call the test_answer function
        return True
    except Exception as e:
        traceback.print_exc()
        return False
 
def run_all_tests_for_answer(question_idx, code, prompt, random_seed=42, tests=None):
    """
    Runs all tests for a given answer and returns the count of passed tests.
    Args:
        question_idx (int): The index of the question for which the tests are being run.
        code (str): The code generated by the model to be tested.
        prompt (str): The prompt provided to the model to generate the code.
        random_seed (int, optional): The random seed for reproducibility. Defaults to 42.
        tests (list, optional): A list of test cases to run. Each test case should be a dictionary containing test details.
    Returns:
        int: The count of tests that passed.
    """
    pass_count = 0
    for test in tests:
        max_tries = 5
        while max_tries > 0:
            max_tries -= 1
            try:
                seed = np.random.randint(10000)
                
                logging.debug(f"Running test for question {question_idx} with model {test['model']} ")
                
                # Parse the code into an AST
                dummy_test_result = run_tests_for_answer(
                    question_idx, # Question index used to find the test.
                    code, # Code generated by the model.
                    prompt,
                    random_seed=seed,
                    test=test)
                if dummy_test_result:
                    pass_count += 1
            except Exception as e:
                traceback.print_exc()
                print(f"Runtime error: {e}")
    return pass_count

